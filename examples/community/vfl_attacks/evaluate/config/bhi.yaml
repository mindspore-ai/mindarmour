# global configuration
dataset: bhi
cuda: 1
log: vfl_bhi
debug: false

# data configuration
target_train_size: -1
target_test_size: -1
backdoor_train_size: 1000
backdoor_test_size: 2000
train_label_size: 70
#target_train_size: 2000
#target_test_size: 2000
#backdoor_train_size: 500
#backdoor_test_size: 200
#train_label_size: 40


# save configuration
save_model: 0
save_data: 0

# load configuration
load_target: 0
load_data: 0
load_model: 0
load_time: 0

# global model configuration
n_passive_party: 1
target_batch_size: 64
target_epochs: 5
# passive party configuration
passive_bottom_model: resnet
passive_bottom_gamma: 0.1
passive_bottom_wd: 0.0005
passive_bottom_momentum: 0.9
passive_bottom_lr: 0.05
passive_bottom_stone: [4,6]
# active party configuration
# active bottom model configuration
active_bottom_model: resnet
active_bottom_gamma: 0.1
active_bottom_wd: 0.0005
active_bottom_momentum: 0.9
active_bottom_lr: 0.05
active_bottom_stone: [4,6]
# active top model configuration
active_top_trainable: 1
active_top_model: fcn
active_top_gamma: 0.1
active_top_wd: 0.0005
active_top_momentum: 0.9
active_top_lr: 0.05
active_top_stone: [4,6]

# backdoor attack global configuration
backdoor: lr_ba
backdoor_label: 0
backdoor_label_size: 41840

# Gradient replacement configuration
g_r_amplify_ratio: 10

# LR-BA attack configuration
lr_ba_alpha: 0.75
lr_ba_active: false
lr_ba_active_r_min: 1.0
lr_ba_active_r_max: 5.0
lr_ba_active_reset: 1.0
lr_ba_top_lr: 0.002
lr_ba_top_momentum: 0.9
lr_ba_top_wd: 0.0001
lr_ba_top_epochs: 25
lr_ba_top_batch_size: 32
lr_ba_top_train_iteration: 1024
lr_ba_top_T: 0.8
lr_ba_top_alpha: 0.75
lr_ba_generate_lr: 0.1
lr_ba_generate_epochs: 100
lr_ba_ema_decay: 0.999
lr_ba_finetune_lr: 0.01
lr_ba_finetune_epochs: 100

lr_ba_top_margin: 0.7
lr_ba_top_lambda_u: 1
lr_ba_top_lambda_u_hinge: 0
lr_ba_train_aug: false
lr_ba_top_temp_change: 1000000
lr_ba_top_co: false
lr_ba_top_separate_mix: false
lr_ba_top_mix_layers_set: [7, 9, 12]
lr_ba_top_batch_size_u: 4

